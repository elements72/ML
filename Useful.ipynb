{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec11a579",
   "metadata": {},
   "source": [
    "Important, at the end check that the code is clean, that all the debug cells are removed (checking shape when not required and so on), follow the numbered structure of the exam with markdown cells, do generalized solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8731b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn_version = sklearn.__version__\n",
    "\n",
    "print(sklearn_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5672746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#At the beginning of every exam import those and then check if something is missing or not useful and delete it\n",
    "\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we have a csv file without header and column names we usually have to do this, but the second method works and is easier\n",
    "\n",
    "data_file = 'name.csv'\n",
    "delimiter = ',' #Check for the delimiter in the file, it could also be ;\n",
    "X = np.loadtxt(data_file, delimiter = delimiter)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "#Or if we have problems with conversion, also this\n",
    "\n",
    "df = pd.read_csv('name.csv', header = None)\n",
    "#In this particular case in order to access the columns of the dataframe we need to do df[num] where num is the 'label' of the\n",
    "#column we want to access, remember not to put it in apices, so not df['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to always store the modifications to dataset, e.g.\n",
    "df.shape\n",
    "df.dropna()\n",
    "df.shape\n",
    "#Won't result in any change because we didn't save the new value of df without Nan values, on the other hand\n",
    "df.shape\n",
    "df = df.dropna()\n",
    "df.shape\n",
    "#Result in a change now because we have stored it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daf0b9",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "\n",
    "# Checking which columns contain numerical data and which contain categorical data.\n",
    "df.dtypes\n",
    "\n",
    "#Structure of numeric data\n",
    "df_num = df.drop(class_label,axis=1) # axis= 1 drops the column\n",
    "df_num.describe()\n",
    "\n",
    "#If we want to have a description also of non numeric data we need to explicitly say\n",
    "cat_attributes = df.dtypes.loc[df.dtypes=='object'].index.values \n",
    "df[cat_attributes].describe()\n",
    "\n",
    "#Histograms\n",
    "df_num.hist(figsize=(10,10));\n",
    "\n",
    "#Boxplot\n",
    "df.boxplot()\n",
    "#Usually boxplot comments are on the range of columns, if much smaller or similar\n",
    "\n",
    "#Pairplot\n",
    "sns.pairplot(df, hue = class_label) #if present\n",
    "#Comments on pairplots are usually looking at the couple of attributes to focus on that visually generate the best data \n",
    "#arrangement to be clustered e.g. The pairplots show that the two most interesting columns are `1` and `2`, their pairplot shows\n",
    "#evident clusters. The pairplots of `0` and `3` show that those columns are uniformly distributed and do not show any pattern.\n",
    "\n",
    "\n",
    "#Scatterplot of itneresting columns of the pairplot\n",
    "interesting_columns = [0,1]\n",
    "plt.scatter(X[:,interesting_columns[0]], X[:,interesting_columns[1]]\n",
    "            , c='white'          # color filling the data markers\n",
    "            , edgecolors='black' # edge color for data markers\n",
    "            , marker='o'         # data marker shape, e.g. triangles (v<>^), square (s), star (*), ...\n",
    "            , s=50)              # data marker size\n",
    "plt.grid()  \n",
    "plt.show()\n",
    "\n",
    "#OR\n",
    "chosen_dim = [0,1]\n",
    "plot_data = pd.DataFrame(X_red)\n",
    "plot_data[class_label] =  y\n",
    "sns.scatterplot(x = chosen_dim[0],y = chosen_dim[1],data=plot_data, hue = class_label);\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3be366",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7099873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing of NaNs values\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "#Data types\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4820c8",
   "metadata": {},
   "source": [
    "### From ordinal/categorical to numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8090ccc",
   "metadata": {},
   "source": [
    "- the **ordinal transformer** generates a mapping from strings to numbers according to the lexicographic sorting of the strings; in this particular case, the strings indicate numeric subranges, and ranges with one digit constitute exceptions\n",
    "        '5-9' happens to be after '20-25'\n",
    "    - it is necessary to transform '5-9' into '05-09', and the same for other similar cases\n",
    "    - a way to do this is to prepare dictionaries for the translation and use the `.map` function\n",
    "    \n",
    "Vedi lab 04 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_features = df.dtypes.loc[df.dtypes == 'object'].index.values\n",
    "print(\"The non-numeric features are:\")\n",
    "print(non_numeric_features)\n",
    "\n",
    "numeric_features = list(set(df.dtypes.index.values)-set(non_numeric_features))\n",
    "print(\"The numeric features are:\")\n",
    "print(numeric_features)\n",
    "\n",
    "ordinal_features =['age', 'tumor-size','inv-nodes'] #This is an example, ordinal features are chosen by hand\n",
    "print(\"The ordinal features are:\")\n",
    "print(ordinal_features)\n",
    "\n",
    "categorical_features = list(set(non_numeric_features) - set(ordinal_features) - set(['Class']))\n",
    "print(\"The categorical features are:\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee4fd4",
   "metadata": {},
   "source": [
    "##### If I have to transform only ordinal to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79212b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus = [2] # list of ordinal attributes\n",
    "transf_dtype = np.int32 # type to be used when converting\n",
    "\n",
    "\n",
    "ordinal_transformer = OrdinalEncoder(dtype = transf_dtype) # we assume the values are encoded so that lexicographic order = intended order\n",
    "df_2 = df.copy()\n",
    "df_2[focus] = ordinal_transformer.fit_transform(df[focus])\n",
    "df_2.head() #just to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ddb3c",
   "metadata": {},
   "source": [
    "##### If I have to transform both ordinal and categorical to numeric at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873df278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transf_dtype = np.int32\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse = False, dtype = transf_dtype)\n",
    "ordinal_transformer = OrdinalEncoder(dtype = transf_dtype)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [('cat', categorical_transformer, categorical_features),\n",
    "                    ('ord', ordinal_transformer, ordinal_features)\n",
    "                   ],\n",
    "                    remainder = 'passthrough'\n",
    "    )\n",
    "\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n",
    "\n",
    "preprocessor.fit(X)\n",
    "print(preprocessor.named_transformers_)\n",
    "\n",
    "X_p = preprocessor.fit_transform(X)\n",
    "df_p = pd.DataFrame(X_p) #just to check\n",
    "df_p.head()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p,y, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f6f41",
   "metadata": {},
   "source": [
    "##### Don't change data type of class label\n",
    "\n",
    "In classification it does not matter what the class actually represents, the learning algorithm treats every class as categorical anyway. In other words whether the names of the classes are strings, characters or numbers does not change anything to the model.\n",
    "\n",
    "It would be a bad idea to use one hot encoding because this would make the problem multi-label. This would make the problem much more complex for the model and would very likely lead to lower performance, or it would require much more data in order to reach the same performance as regular classification. This is because there are much more combinations possible in the multi-label problem, and in this case this higher level of complexity is pointless since there can be only one class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396f3ec",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from functools import partial\n",
    "\n",
    "# k = num, num is the number of the best features that will be selected (e.g. if I start from 4 features k must be <4)\n",
    "\n",
    "kbest = SelectKBest(score_func=partial(mutual_info_classif,random_state=random_state), k=num) \n",
    "\n",
    "fit = kbest.fit(X,y)\n",
    "X_reducted = fit.transform(X)\n",
    "X_reducted.shape\n",
    "\n",
    "#Other selection methods are in the lab about feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169ee72",
   "metadata": {},
   "source": [
    "Features ranking with RFE (Recursive Feature elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "# feature extraction\n",
    "rfe = RFE(estimator, n_features_to_select=5)\n",
    "fit = rfe.fit(X, y)\n",
    "X_reducted = fit.transform(X)\n",
    "\n",
    "print(\"Feature Ranking: %s\", fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a64c84",
   "metadata": {},
   "source": [
    "Feature selection unsupervised with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "print(\"Explained Variance:\", fit.explained_variance_ratio_)\n",
    "X_reducted = fit.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f27b5b",
   "metadata": {},
   "source": [
    "### Rescaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826db01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc640c",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Hyperparameter tuning with cross-validation (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b296b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df[target_name].unique()\n",
    "classes.sort()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ec342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model_lbls = [\n",
    "#              'dt', \n",
    "#              'nb', \n",
    "#              'lp', \n",
    "#              'svc', \n",
    "             'knn',\n",
    "            ]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_param_dt = [{'max_depth': [range(1,20)]}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "scores = ['precision', #'recall', recall_macro, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores and classification report\n",
    "\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246634ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_short = {}\n",
    "\n",
    "for score in scores:\n",
    "    print('='*40)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls:\n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                           scoring='%s_macro' % score, \n",
    "                           iid = False, \n",
    "                           return_train_score = False,\n",
    "                           n_jobs = 2, # this allows using multi-cores\n",
    "                           )\n",
    "        clf.fit(X_train, y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3be705",
   "metadata": {},
   "source": [
    "##### Doing separatedly model 1 and model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters to be explored by GridSearchCV for the decision tree model.\n",
    "tuned_param_dt = [{\"max_depth\": list(range(1, 16))}]\n",
    "\n",
    "# Setting the parameters to be explored by GridSearchCV for the k nearest neighbor model.\n",
    "tuned_param_knn =[{\"n_neighbors\": list(range(1, 16))}]\n",
    "\n",
    "# Defining the models to be fitted.\n",
    "models = {\n",
    "    \"Model1\": {\"name\": \"Decision Tree\",      \"estimator\": DecisionTreeClassifier(), \"param\": tuned_param_dt},\n",
    "    \"Model2\": {\"name\": \"K Nearest Neighbor\", \"estimator\": KNeighborsClassifier(),   \"param\": tuned_param_knn}\n",
    "}\n",
    "\n",
    "# Defining the list of scores to be explored.\n",
    "score = \"recall_macro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b41907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function which takes as input the fitted model and returns the classification report.\n",
    "def print_results(name, model, y_test, y_pred):\n",
    "    \n",
    "    # Printing the type of model.\n",
    "    print(\"Model name: {}.\".format(name))\n",
    "    \n",
    "    # Printing the grid scores obtained during training.\n",
    "    print(\"\\nGrid scores:\")\n",
    "    \n",
    "    # Iterating over the grid rows.\n",
    "    for mean, std, params in zip(model.cv_results_[\"mean_test_score\"], model.cv_results_[\"std_test_score\"], model.cv_results_[\"params\"]):\n",
    "        \n",
    "        # Printing one row of the grid.\n",
    "        print(\"{:.3f} (+/- {:.3f}) for {}\".format(mean, std * 2, params))\n",
    "    \n",
    "    # Printing the best parameters set.\n",
    "    print(\"\\nBest parameters set: {}.\".format(model.best_params_))\n",
    "    \n",
    "    # Printing the detailed classification report.\n",
    "    print(\"\\nDetailed classification report:\\n{}\".format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for the first model \n",
    "# Activating the grid search.\n",
    "clf = GridSearchCV(models[\"Model1\"][\"estimator\"], models[\"Model1\"][\"param\"], scoring = score, cv = 5)\n",
    "\n",
    "# Fitting the model.\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda1f89",
   "metadata": {},
   "source": [
    "##### Best hyperparameters if report is not requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best found hyperparameters are: {}\".format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b18eb",
   "metadata": {},
   "source": [
    "##### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for the first model\n",
    "# Computing predictions with the test set.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Printing the results.\n",
    "print_results(models[\"Model1\"][\"name\"], clf, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce2c95",
   "metadata": {},
   "source": [
    "##### Confusion matrix and accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy on test set is {:.2f}%\".format(acc*100))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "#Different confusion matrix normalizing for each class\n",
    "plot_confusion_matrix(clf, X_test, y_test, normalize='pred');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d3a737",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fcca8",
   "metadata": {},
   "source": [
    "When have to decide between K-means and DBSCAN see:\n",
    "1. If in the pairplot of choosen data there are convex distributions or outliers (if yes select DBSCAN)\n",
    "2. If clusters vary a lot in densities (if yes select K-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc0082",
   "metadata": {},
   "source": [
    "### K-means\n",
    "In k-means the best number of cluster (the only hyperparameter) can be selected via elbow method by looking at the graph of distortions and silhouette scores of estimators fitted with different values of k.\n",
    "If this is not enough a visual inspection of clustered data can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "n_cluster_range = range(2,10)\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clust in n_cluster_range:\n",
    "\n",
    "    km = KMeans(n_clusters = n_clust,random_state = random_state)\n",
    "    \n",
    "    \n",
    "    y_km = km.fit_predict(X)\n",
    "    distortion = km.inertia_\n",
    "    silhouette = silhouette_score(X,y_km)\n",
    "    \n",
    "    distortions.append(distortion)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    \n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('Inertia', color=color)\n",
    "ax1.plot(n_cluster_range, distortions, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Silhouette scores', color=color)  \n",
    "ax2.plot(n_cluster_range, silhouette_scores, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0,1) \n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_k = #k chosen before via elbow method\n",
    "km = KMeans(n_clusters=good_k, \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=random_state)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "#Silhouette score of the best parameter choice\n",
    "print(silhouette_score(X,y_km))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad77fd",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "With DBSCAN if I don't have supervised information on class labels I can tune the hyperparameters (n_clusters, epsilon and minPoints) via GridSearch, plotting then a report and manually choosing the best one with a tradeoff between silhouette score and unclustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c46478",
   "metadata": {},
   "source": [
    "For a good starting approximation of min_points and eps we do as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all if we have a dataframe we transform it into a numpy array just for the sake of the following two cells\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For min_points we look around twice the number of attributes of the dataset (X.shape[1])\n",
    "min_points=2*X.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77862dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eps we look at the elbow curve of the following graph and we range in its proximity (in this case it was 0.2)\n",
    "k_distances = []\n",
    "for i in range(0, X.shape[0]):\n",
    "    k_point_distances = []\n",
    "    for j in range(0, X.shape[0]):\n",
    "        if i!=j:\n",
    "            dist = np.sqrt(sum((X[i,:]-X[j,:])**2))\n",
    "            k_point_distances.append(dist)\n",
    "    k_point_distances.sort()\n",
    "    k_distances.append(k_point_distances[min_points-1])\n",
    "    \n",
    "k_distances.sort(reverse=True)\n",
    "plt.plot(range(0,len(k_distances)), k_distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace63b5",
   "metadata": {},
   "source": [
    "ATTENTION, the last cell is the ParameterGrid in case of already choosen number of clusters (e.g. in case of supervised label), the one that follows is the more general one in which number of clusters is an hyperparameter free to be tuned\n",
    "\n",
    "ATTENTION if the thresholds are set too high (the sil one) or too low (the unc one) no result will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version with tuning also of the number of clusters hyperparameter, not previously fixed\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "grid = {'eps':list(np.arange(0.1 ,0.3, 0.01)), 'min_samples':list(range(min_points-4,min_points+4,1))}\n",
    "params = list(ParameterGrid(grid))\n",
    "sil_thr = 0.56  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 10    # visualize results only for combinations with uncl_ratio below the threshold\n",
    "columns = [\"eps\", \"min_samples\", \"n_clusters\", \"silhouette\", \"unclust%\"]\n",
    "result = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "\n",
    "for i in params:\n",
    "    grid_est = DBSCAN(eps=i['eps'], min_samples=i['min_samples'])\n",
    "    yge = grid_est.fit_predict(X)\n",
    "    labels = np.unique(yge)\n",
    "    if -1 in labels:\n",
    "        n=1\n",
    "    else:\n",
    "        n=0\n",
    "    uncl_ratio = np.count_nonzero(yge==-1)/len(yge)*100\n",
    "    n_clusters = np.count_nonzero(labels>-1)\n",
    "    y_no_noise = yge[yge!=-1]\n",
    "    \n",
    "    if len(labels)-n < 2:\n",
    "        sil_score=-1\n",
    "        sdi=-1\n",
    "    else:\n",
    "        sil_score = silhouette_score(X[yge!=-1], y_no_noise)\n",
    "        \n",
    "    if sil_score>sil_thr and uncl_ratio < unc_thr:\n",
    "        result = result.append({ 'eps': i[\"eps\"],\n",
    "                                'min_samples':i[\"min_samples\"],\n",
    "                                'n_clusters':n_clusters,\n",
    "                                'silhouette':sil_score,\n",
    "                                'unclust%':uncl_ratio}, ignore_index=True)\n",
    "        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d643cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version with an already fixed number of clusters (i.e. required_clusters) equal to the number of classes in the supervised \n",
    "#column\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "grid = {'eps':list(np.arange(0.1 ,0.3, 0.01)), 'min_samples':list(range(min_points-4,min_points+4,1))}\n",
    "params = list(ParameterGrid(grid))\n",
    "sil_thr = 0.56  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 10    # visualize results only for combinations with uncl_ratio below the threshold\n",
    "columns = [\"eps\", \"min_samples\", \"n_clusters\", \"silhouette\", \"unclust%\"]\n",
    "result = pd.DataFrame(columns=columns)\n",
    "required_clusters = len(np.unique(y))\n",
    "\n",
    "\n",
    "for i in params:\n",
    "    grid_est = DBSCAN(eps=i['eps'], min_samples=i['min_samples'])\n",
    "    yge = grid_est.fit_predict(X)\n",
    "    labels = np.unique(yge)\n",
    "    if -1 in labels:\n",
    "        n=1\n",
    "    else:\n",
    "        n=0\n",
    "    uncl_ratio = np.count_nonzero(yge==-1)/len(yge)*100\n",
    "    n_clusters = np.count_nonzero(labels>-1)\n",
    "    y_no_noise = yge[yge!=-1]\n",
    "    \n",
    "    if len(labels)-n < 2:\n",
    "        sil_score=-1\n",
    "        sdi=-1\n",
    "    else:\n",
    "        sil_score = silhouette_score(X[yge!=-1], y_no_noise)\n",
    "        \n",
    "    if n_clusters==required_clusters and sil_score>sil_thr and uncl_ratio < unc_thr:\n",
    "        result = result.append({ 'eps': i[\"eps\"],\n",
    "                                'min_samples':i[\"min_samples\"],\n",
    "                                'n_clusters':n_clusters,\n",
    "                                'silhouette':sil_score,\n",
    "                                'unclust%':uncl_ratio}, ignore_index=True)\n",
    "        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bbef5",
   "metadata": {},
   "source": [
    "Manually choose the best set of hyperparameters looking at: silhouette score, uncluster % and number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_hyper_db = {'eps':0.18, 'min_samples':4}\n",
    "dbscan = DBSCAN(eps = chosen_hyper_db['eps'], min_samples=chosen_hyper_db['min_samples'])\n",
    "y_db = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339af1a",
   "metadata": {},
   "source": [
    "### Showing a pairplot of the clustered data\n",
    "This can be done to visually check the best number of clusters and not only rely on parametergrid for DBSCAN or elbow method for K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6251c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = y_km # or y_db\n",
    "\n",
    "sns.pairplot(df, hue = 'cluster');\n",
    "\n",
    "#Or if I want to show only interesting columns \n",
    "\n",
    "sns.pairplot(df[int_cols + ['cluster']], vars = df[int_cols], hue = 'cluster');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7a747",
   "metadata": {},
   "source": [
    "### Assessing cluster quality via silhouette plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plot_silhouette function in the plot_silhouette.py in useful\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0] # it is the number of rows\n",
    "# Compute the Silhouette Coefficient for each sample, with the euclidean metric\n",
    "silhouette_score_samples = silhouette_samples(X, y_km, metric='euclidean')\n",
    "plt.title('Silhouette score for samples with {} clusters'.format(good_k))\n",
    "plot_silhouette(silhouette_score_samples, y_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be319aeb",
   "metadata": {},
   "source": [
    "### Using y supervised column as gold standard\n",
    "In order to correcly have an estimate of the accuracy, clusters (as well as noise points) will be mapped to the highest class they represent\n",
    "Then I will compute accuracy score between prediction and gold standard and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4834bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def remap(y_true, y_pred):\n",
    "    y_mapped = y_pred.copy()\n",
    "    for lab in np.unique(y_pred):\n",
    "        true_l, count = np.unique(y_true[y_pred==lab], return_counts=True)\n",
    "        y_mapped[y_pred==lab] = true_l[np.argmax(count)]\n",
    "    return y_mapped \n",
    "\n",
    "y_db_rem = remap(y, y_db) #or y_km\n",
    "print(\"The accuracy is: {:.2f}%\".format(accuracy_score(y,y_db_rem)*100))\n",
    "print(\"The confusion matrix:\")\n",
    "print(confusion_matrix(y,y_db_rem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca74cc",
   "metadata": {},
   "source": [
    "### A sorted list of the discovered clusters for decreasing sizes\n",
    "I list the number of data points inside each cluster, in this case in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(y_km)\n",
    "counts = pd.DataFrame(labels.value_counts(), columns=['Count']).reset_index()\n",
    "counts = counts.rename(columns = {0: 'Cluster'})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c123b",
   "metadata": {},
   "source": [
    "### If asked to show  the plot of the global inertia and silhouette index for the parameters under examination in case of DBSCAN see ml_lab_exam_simulation_dbscan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
